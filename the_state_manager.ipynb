{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXQ7r10gfp11ype+xaLmoq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/louisdennington-design/decision-tree-dissertation/blob/main/the_state_manager.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Mount Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ],
      "metadata": {
        "id": "O8nDQhbiIU1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import packages\n",
        "\n",
        "pip install streamlit\n",
        "pip install streamlit-chat\n",
        "\n",
        "import os\n",
        "import json\n",
        "import streamlit as st\n",
        "from streamlit_chat import message"
      ],
      "metadata": {
        "id": "DYIc3r4HIXzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global parameters\n",
        "\n",
        "LOAD_PATH = \"/content/drive/My Drive/Colab Notebooks/Dissertation/JSON\"\n",
        "LOAD_FILE = os.path.join(LOAD_PATH, \"guideline_structured.json\")\n",
        "\n",
        "SAVE_PATH = \"DEFINE\"\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)\n",
        "SAVE_FILE = os.path.join(SAVE_PATH, \"DEFINE\")\n",
        "\n",
        "PATIENT_DICTIONARY = {}"
      ],
      "metadata": {
        "id": "aOR4djvwIaeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load LLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\")"
      ],
      "metadata": {
        "id": "M4JmHPGXT-Qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itw3VugfBiWy"
      },
      "outputs": [],
      "source": [
        "# Load JSON\n",
        "\n",
        "def load_json():\n",
        "    try:\n",
        "        with open(LOAD_FILE, \"r\", encoding=\"utf-8\") as guideline_structured:\n",
        "            return json.load(guideline_structured)\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(f'JSON file not found: {LOAD_FILE}')\n",
        "\n",
        "guideline_structured = load_json()\n",
        "\n",
        "print(type(guideline_structured))\n",
        "print(len(guideline_structured))\n",
        "print(guideline_structured[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create record of exact guideline version extracted with metadata\n",
        "\"\"\"In case of future runs with differernt guidelines and structure\n",
        "Then problems with a run can be traced precisely\"\"\"\n",
        "- guideline name\n",
        "- html\n",
        "- scrape date"
      ],
      "metadata": {
        "id": "jXvkbEtTDhNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save local copy of JSON with metadata\n",
        "- protect the file somehow from being overwritten or edited?"
      ],
      "metadata": {
        "id": "QY2f6B38EKFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an index or summary for the JSON to enable more reliable searching by the state manager?"
      ],
      "metadata": {
        "id": "ksFqOfMZEghF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### NOTE\n",
        "\"\"\"\n",
        "If the question being asked tessellates with a particular section (e.g., a medication)\n",
        "it may be worth flagging other areas (e.g., carer support.. physical health...)\n",
        "The model may need to supply back to the user a list of \"have you also considered...\" making use of these headings\n",
        "to prevent a narrow focus on the main content of the question\n",
        "and ensure that other recommendation sections are also being considered\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "zpCccgHf-4fO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_message():\n",
        "    \"\"\"\n",
        "    Uses a UI like Streamlit or an API to send and receive messages\n",
        "    \"\"\"\n",
        "\n",
        "    # From https://www.geeksforgeeks.org/python/create-a-chatbot-with-openai-and-streamlit-in-python/\n",
        "    st.title(&quot;NICE GUIDELINE CHATBOT&quot;)\n",
        "    if 'user_input' not in st.session_state:\n",
        "        st.session_state['user_input'] = []\n",
        "\n",
        "    if 'openai_response' not in st.session_state:\n",
        "        st.session_state['openai_response'] = []\n",
        "\n",
        "    def get_text():\n",
        "        input_text = st.text_input(&quot;Enter your question and patient information here:&quot;, key=&quot;input&quot;)\n",
        "        return input_text\n",
        "\n",
        "    user_input = get_text()\n",
        "\n",
        "    if user_input:\n",
        "        output = api_calling(user_input)\n",
        "        output = output.lstrip(&quot;\\n&quot;)\n",
        "\n",
        "        # Store the output\n",
        "        st.session_state.openai_response.append(user_input)\n",
        "        st.session_state.user_input.append(output)\n"
      ],
      "metadata": {
        "id": "y7eaW6NDQNgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_llm(prompt):\n",
        "    \"\"\"\n",
        "    General function for any call to the LLM\n",
        "    What is passed to the LLM (\"prompt\") should be decided by other functions\n",
        "    \"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt,\n",
        "                       return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    outputs = model.generate(**inputs,\n",
        "                             max_new_tokens=500,\n",
        "                             do_sample=False) # deterministic decoding without random sampling\n",
        "                                            # if removed, reinstate temperature / top_p / top_k\n",
        "\n",
        "    llm_response = tokenizer.batch_decode(outputs[:, inputs[\"input_ids\"].shape[1]:],\n",
        "                                          skip_special_tokens=True)\n",
        "\n",
        "    return llm_response[0]"
      ],
      "metadata": {
        "id": "0KF92-48cSRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_patient_facts(user_input):\n",
        "    \"\"\"\n",
        "    Use LLM to decompose message into fields captured by JSON-keys\n",
        "    \"\"\"\n",
        "\n",
        "    user_input = \"USER INPUT: \" + (None)\n",
        "\n",
        "    prompt = \"\"\"You are extracting structured information from USER INPUT.\n",
        "\n",
        "                RULES:\n",
        "                - 'age' should be an integer\n",
        "                - 'gender' should be 'male', 'female' or 'other'\n",
        "                - 'medications' should be a dictionary of medicaitons limited to known medication names, with duration taken also recorded\n",
        "                - 'recent_medical_events'\n",
        "                - 'patient_preferences'\n",
        "\n",
        "                Produce JSON with exactly these keys:\n",
        "                - age\n",
        "                - gender\n",
        "                - medications\n",
        "                - recent_medical_events\n",
        "                - patient_preferences\n",
        "                \"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt,\n",
        "                       return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    outputs = model.generate(**inputs,\n",
        "                             max_new_tokens=500,\n",
        "                             do_sample=False) # deterministic decoding without random sampling\n",
        "                                            # if removed, reinstate temperature / top_p / top_k\n",
        "\n",
        "    patient_facts = tokenizer.batch_decode(outputs[:, inputs[\"input_ids\"].shape[1]:],\n",
        "                                          skip_special_tokens=True)\n",
        "\n",
        "    return patient_facts[0]"
      ],
      "metadata": {
        "id": "OUKvTzrzZqIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_relevant_recommendations(patient_facts):\n",
        "    \"\"\"\n",
        "    Based on matches between decomposed user input and guideline_structured.json\n",
        "    \"\"\"\n",
        "\n",
        "    matching_fields = [] # A data object capturing all fields shared between decomposed user input and the structured guideline\n",
        "\n",
        "    relevant_recommendations = [] # A place to store the full text of original recommendations\n",
        "\n",
        "    for i, v in patient_facts: # Iterate over each key-value pair in patient_facts\n",
        "        if v in guideline_structured: # Check whether the value appears in the json\n",
        "            matching_fields.append(i, v) # Add that value to \"matching_fields\"\n",
        "\n"
      ],
      "metadata": {
        "id": "l_4yp2BgQb_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decide_next_question_or_answer():\n",
        "    \"\"\"\n",
        "    - must record history of decisions taken for audit\n",
        "    \"\"\"\n"
      ],
      "metadata": {
        "id": "niYj51LZcjYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_question():\n",
        "    # Needed as separate step?"
      ],
      "metadata": {
        "id": "pnAyZyVVimUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_patient_dict():\n",
        "    \"\"\"\n",
        "    - what is known about the \"patient\" object\n",
        "    - history of questions asked and user answers, or is this in the UI?\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "i99Afg6ucicF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}