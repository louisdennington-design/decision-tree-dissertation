{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN4Y+jhk+OgXL08lilTASLU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/louisdennington-design/decision-tree-dissertation/blob/main/llm_makes_json.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AE9TItDTttR9"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ],
      "metadata": {
        "id": "tpxWJWQBwyad"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set base parameters\n",
        "\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "LOAD_PATH = \"/content/drive/My Drive/Colab Notebooks/Dissertation/Scrapes\"\n",
        "LOAD_FILE = os.path.join(LOAD_PATH, \"bipolar_scrape.json\")\n",
        "\n",
        "SAVE_PATH = \"/content/drive/My Drive/Colab Notebooks/Dissertation/JSON\"\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)\n",
        "SAVE_FILE = os.path.join(SAVE_PATH, \"bipolar.json\")"
      ],
      "metadata": {
        "id": "tO5o8yykwzaN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load LLM\n",
        "\n",
        "\"\"\"\n",
        "Focus is on instruction-following models from Hugging Face\n",
        "With free licence (Apache)\n",
        "Qwen seems to have been trained on producing JSON formats\n",
        "Parameters are good balance between small and big\n",
        "Shoul also check Llama offerings\n",
        "\"\"\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\")"
      ],
      "metadata": {
        "id": "wzBIc0pGrsre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "\n",
        "## Should also carry out test prompt of transforming recommendations\n",
        "\n",
        "text = \"Should someone with a diagnosis of bipolar who is taking lithium be referred to secondary care if they are mildly irritable?\"\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=200)\n",
        "\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "L13yt0E5NlfM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5a06ae5-533c-4fa5-889e-472fc31dfa6e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Should someone with a diagnosis of bipolar who is taking lithium be referred to secondary care if they are mildly irritable? The answer is yes, because:\n",
            "A: Lithium levels may be elevated\n",
            "B: They are at risk of developing mania\n",
            "C: Lithium toxicity is more likely in irritable patients\n",
            "D: Lithium levels may be decreased\n",
            "\n",
            "The correct answer is A: Lithium levels may be elevated.\n",
            "\n",
            "Explanation:\n",
            "Irritability can be a side effect of lithium, and it's important for healthcare providers to monitor this symptom. If a patient with bipolar disorder who is on lithium therapy is experiencing mild irritability, it could indicate that their lithium levels are elevated. Elevated lithium levels can lead to toxicity, which has various adverse effects including gastrointestinal symptoms, tremor, confusion, and in severe cases, life-threatening complications.\n",
            "\n",
            "While the other options (B, C, and D) might also be relevant considerations, they do not directly address the primary concern of irritability as a potential sign of lithium level changes. Here's a brief overview of why the other options are less directly applicable:\n",
            "\n",
            "-\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load JSON of raw recommendations\n",
        "\n",
        "def load_json():\n",
        "    try:\n",
        "        with open(LOAD_FILE, \"r\", encoding=\"utf-8\") as raw_recommendations:\n",
        "            return json.load(raw_recommendations)\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(f'JSON file not found: {LOAD_FILE}')\n",
        "\n",
        "raw_recommendations = load_json()\n",
        "\n",
        "print(type(raw_recommendations))\n",
        "print(len(raw_recommendations))\n",
        "print(raw_recommendations[0])"
      ],
      "metadata": {
        "id": "TMTjqJSd6IT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def extract_one_recommendation():\n",
        "\n",
        "        for item in raw_rec:\n",
        "\n",
        "        heading_1 = []\n",
        "        sub_heading_1 = []\n",
        "        sub_heading_2 = []\n",
        "        r_number = []\n",
        "        r_text = []\n",
        "\n",
        "        heading_1.append(item[\"heading_1\"])\n",
        "        sub_heading_1.append(item[\"sub_heading_1\"])\n",
        "        if item.get(\"sub_heading_2\") != None:\n",
        "            sub_heading_2.append(item[\"sub_heading_2\"])"
      ],
      "metadata": {
        "id": "e_sxR4kh1Fcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt LLM to translate raw recommendations into reasoning format\n",
        "\n",
        "## Check OpenAI prompt engineering guide\n",
        "\n",
        "def construct_prompt():\n",
        "\n",
        "    \"\"\"\n",
        "    Given one recommendation entry {}, creates the prompt to extract one normalised JSON item\n",
        "    \"\"\"\n",
        "\n",
        "    heading_context = \" > \".join(heading_1, sub_heading_1, sub_heading_2)\n",
        "\n",
        "    return f\"\"\"\n",
        "    You are extracting structured information from a NICE guideline recommendation.\n",
        "\n",
        "    RULES:\n",
        "    - output must be valid JSON only (no markdown)\n",
        "    - do not invent clinical information, thresholds or populations; use only what is present\n",
        "    - 'necessity' is language such as 'should', 'must', 'consider'\n",
        "    - actions are concrete imperatives\n",
        "    - if there is more than one action, retain all\n",
        "    - use 'null' if the information is not explicit in the recommendation or heading\n",
        "\n",
        "    CONTEXT: {heading_context}\n",
        "\n",
        "    RECOMMENDATION NUMBER: {r_number}\n",
        "    RECOMMENDATION TEXT:M {r_text}\n",
        "\n",
        "    Produce JSON with exactly these keys:\n",
        "    - action\n",
        "    - necessity\n",
        "    - scope\n",
        "    - population\n",
        "    - temporality\n",
        "    - exceptions\n",
        "    - original_recommendation_number\n",
        "    - original_recommendation_text\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "LPfVqWLjwkp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_llm_on_entity(tokenizer, model, entity,):\n",
        "\n",
        "    \"\"\"\n",
        "    Call the model on a single prompt using the prompt function\n",
        "    Return model response\n",
        "    \"\"\"\n",
        "\n",
        "    for i in enumerate(json_data):\n",
        "\n",
        "        prompt = construct_prompt(entity)\n",
        "\n",
        "        temp = run_llm_on_entity()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g4i10M6L6HUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_output_to_true_json():\n",
        "    \"\"\"\n",
        "    Takes output from run_llm_on_entity\n",
        "    Turns it into a true JSON dictionary\n",
        "    Validation to check correctness - though should this be separate function?\n",
        "    \"\"\"\n",
        "\n",
        "    strip text\n",
        "    convert to json\n",
        "    deal with failure\n",
        "\n",
        "    VALIDATION\n",
        "    check strings\n",
        "    check null or none\n",
        "    r_text and r_number present for all entries?\n"
      ],
      "metadata": {
        "id": "6JNEcneM4_9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vyhNkqnC6eLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json.dump(DATA_VARIABLE, open(SAVE_FILE.replace(\".txt\", \".json\"), \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n"
      ],
      "metadata": {
        "id": "poFitsKb5uYx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}